Upraven√© soubory pro odstranƒõn√≠ posesiv a kontrolu d√©lky vƒõt
1Ô∏è‚É£ B_core/templates/segment_prompt.txt
<artifact identifier="segment-prompt-updated" type="application/vnd.ant.code" language="plaintext" title="B_core/templates/segment_prompt.txt (Updated)">
ROLE & INTENT
You are a professional historian and long-form narrator. Write an idiomatic, fluent {LANG} voiceover paragraph that is 100% factual and source-grounded. No dialogue, no lists, no SSML, third-person only.
SERIES & EPISODE CONTEXT

Series Title: {SERIES_TITLE}
Total Episodes: {TOTAL_EPISODES}
Episode Number: {EPISODE_NUMBER}
Episode Title: {EPISODE_TITLE}
Episode Description (1‚Äì2 lines): {EPISODE_DESCRIPTION}

THIS SEGMENT (MSP FOCUS)

Segment Index: {SEGMENT_INDEX} of {SEGMENTS_TOTAL}
MSP Label (exact focus): {MSP_LABEL}
Minutes Target: {MINUTES_TARGET}
Target Words: {WORD_TARGET} (¬±{WORD_TOLERANCE_PERCENT}%)
Coverage rule: Cover only this MSP. Do not borrow content from other MSPs. If continuity is needed, use exactly one brief bridging sentence.

TRANSITIONS FOR SMOOTH STITCHING (MANDATORY)

Opening Hook: 1‚Äì2 short sentences that fluidly pick up the narrative flow from the previous segment without repeating its facts.
Closing Handoff: exactly 1 sentence that gently primes the listener for the next segment without revealing its details.

LANGUAGE REQUIREMENTS

Entire output in {LANG}, idiomatic and natural as written by a native historian.
Numbers style: {NUMBERS_STYLE} (recommended: digits for years; words elsewhere).
Balanced sentence length; avoid abrupt stops and overly long sentences.

CRITICAL STYLE REQUIREMENTS (MUST FOLLOW)

Use OBJECTIVE THIRD-PERSON voice throughout
NEVER use possessive first-person plural pronouns:

English: "our", "my", "ours"
Czech: "n√°≈°", "na≈°e", "na≈°eho", "na≈°em", "na≈°√≠"
German: "unser", "unsere", "unserem"
Spanish: "nuestro", "nuestra", "nuestros"
French: "notre", "nos"


Refer to the series by name: "the documentary series X" or "the series X" NOT "our series"
Each sentence MUST be 15-30 words maximum
If a sentence would naturally exceed 30 words, split it at a comma, semicolon, or conjunction
Be engaging but maintain objective, documentary tone
Avoid phrases like "we will see", "we will explore", "let us examine"

FACTUALITY & SOURCING

Use established consensus; mention uncertainties only if essential and briefly.
Base on these sources (subset relevant to this MSP): {SOURCES_SEGMENT}
No speculation, no invented entities, no anachronisms.

TTS-FRIENDLY RULES (ElevenLabs)

Output body = one single paragraph; no headings, no bullets, no markup.
Use punctuation for natural pauses; avoid parentheses and quotes where possible.

OPTIONAL REFERENCE (DO NOT OUTPUT)
{CANON_BLOCK}
OUTPUT FORMAT (STRICT)
Return exactly:

The narration paragraph for this segment (one single paragraph).
A line with three dashes.
VALIDATION in YAML:

---VALIDATION---
est_wordcount: <integer>
opening_hook_present: yes|no
closing_handoff_present: yes|no
segment_focus_covered: yes|no
overlap_with_other_msps: none|minor|major
language_check: native-like|needs-rewrite
max_sentence_length: <integer - longest sentence in words>
possessive_pronouns_used: yes|no
notes: <one short sentence if any risk found>
</artifact>

2Ô∏è‚É£ historical_processor/narration_core/generator.py
<artifact identifier="generator-updated" type="application/vnd.ant.code" language="python" title="historical_processor/narration_core/generator.py (Updated)">
# -*- coding: utf-8 -*-
from __future__ import annotations
from typing import Dict, Optional
from dataclasses import asdict
from openai import OpenAI
from .types import EpisodeContext, GeneratedText, GeneratorConfig
from .cache import NarrationCache
INTRO_PROMPT_VERSION = "v2"
TRANSITION_PROMPT_VERSION = "v2"
class IntroGenerator:
def init(self, api_key: str, cfg: Optional[GeneratorConfig] = None):
self.client = OpenAI(api_key=api_key)
self.cfg = cfg or GeneratorConfig()
self.cache = NarrationCache()
def generate(self, ctx: EpisodeContext) -> GeneratedText:
    lang_names = {
        'CS': 'Czech',
        'EN': 'English',
        'DE': 'German',
        'ES': 'Spanish',
        'FR': 'French'
    }
    lang_full = lang_names.get(ctx.language, 'Czech')

    series_context_text = '\n'.join(f"- {item}" for item in ctx.series_context)
    episode_desc_text = '\n'.join(f"- {item}" for item in ctx.episode_description)

    user_prompt = f"""You are an experienced documentary narrator and editor. Create a compelling introduction for a historical documentary episode.
SERIES INFORMATION:

Title: {ctx.series_title}
Context:
{series_context_text}

EPISODE INFORMATION:

Episode {ctx.episode_index} of {ctx.total_episodes}
Title: {ctx.episode_title}
Description:
{episode_desc_text}

TASK:
Write a 5-6 sentence introduction in {lang_full} that:

Situates this episode within the overall series
Highlights ONLY the main theme of this episode (no details)
Creates anticipation for what follows
Uses documentary-style, calm, professional narration
Is written for text-to-speech (TTS) - clear, flowing sentences

CRITICAL STYLE RULES (MUST FOLLOW):

Use OBJECTIVE THIRD-PERSON voice throughout
NEVER use possessive first-person plural pronouns:

English: "our", "my", "ours"
Czech: "n√°≈°", "na≈°e", "na≈°eho", "na≈°em", "na≈°√≠"
German: "unser", "unsere", "unserem"
Spanish: "nuestro", "nuestra", "nuestros"
French: "notre", "nos"


Refer to the series by name: "the documentary series {ctx.series_title}" or "the series {ctx.series_title}" NOT "our series"
Each sentence MUST be 15-30 words maximum
If a sentence would naturally exceed 30 words, split it at a comma or conjunction
Avoid phrases like "we will see", "we will explore", "let us examine"
Be engaging but maintain objective tone

IMPORTANT:

Write ONLY the introduction text, nothing else
No metadata, no labels, no explanations
Each sentence should be 15-30 words maximum
Use natural, documentary language
Be engaging but not sensationalist"""
  payload = {
      "type": "intro",
      "version": INTRO_PROMPT_VERSION,
      "language": ctx.language,
      "series_title": ctx.series_title,
      "episode_title": ctx.episode_title,
      "episode_index": ctx.episode_index,
      "total_episodes": ctx.total_episodes,
      "episode_desc_preview": episode_desc_text[:500],
  }
  cached = self.cache.load(payload)
  if cached:
      return GeneratedText(text=cached.get("text", ""), provenance="gpt", prompt_hash=cached.get("cache_key"))

  try:
      response = self.client.chat.completions.create(
          model=self.cfg.model,
          messages=[
              {"role": "system", "content": "You are a professional documentary narrator and editor specializing in historical content. You always use objective third-person voice and never use possessive pronouns like 'our' or 'my'."},
              {"role": "user", "content": user_prompt},
          ],
          temperature=self.cfg.temperature_intro,
          max_tokens=self.cfg.max_tokens_intro,
      )
      text = (response.choices[0].message.content or "").strip()
      cache_key = self.cache.save(payload, {"text": text})
      return GeneratedText(text=text, provenance="gpt", prompt_hash=cache_key)
  except Exception as e:
      # Fallback very simple intro (without possessives)
      fallback = f"{ctx.episode_title} je souƒç√°st√≠ s√©rie {ctx.series_title}." if ctx.language == 'CS' else f"{ctx.episode_title} is part of the series {ctx.series_title}."
      return GeneratedText(text=fallback, provenance="gpt", prompt_hash=None, meta={"error": str(e)})


class TransitionGenerator:
def init(self, api_key: str, cfg: Optional[GeneratorConfig] = None):
self.client = OpenAI(api_key=api_key)
self.cfg = cfg or GeneratorConfig()
self.cache = NarrationCache()
def generate(self, prev_segment: str, next_segment: str, language: str) -> GeneratedText:
    lang_names = {
        'CS': 'Czech',
        'EN': 'English',
        'DE': 'German',
        'ES': 'Spanish',
        'FR': 'French'
    }
    lang_full = lang_names.get(language, 'Czech')

    prev_end = ' '.join(prev_segment.split()[-150:])
    next_start = ' '.join(next_segment.split()[:150])

    user_prompt = f"""You are an experienced documentary narrator creating smooth transitions between segments.
PREVIOUS SEGMENT (ending):
{prev_end}
NEXT SEGMENT (beginning):
{next_start}
TASK:
Write a 1-2 sentence transition in {lang_full} that:

Smoothly connects the previous topic to the next using at least one concrete anchor (time/entity/keyword) present in the context
Maintains chronological and thematic continuity
Uses neutral, documentary style
Does NOT summarize or remove content
Avoids meta phrases and does not add new facts
Is written for text-to-speech (TTS)

CRITICAL STYLE RULES (MUST FOLLOW):

Use OBJECTIVE THIRD-PERSON voice throughout
NEVER use possessive pronouns:

English: "our", "my", "ours"
Czech: "n√°≈°", "na≈°e", "na≈°eho", "na≈°em", "na≈°√≠"
German: "unser", "unsere", "unserem"
Spanish: "nuestro", "nuestra"
French: "notre", "nos"


Each sentence MUST be 14-28 words maximum
Avoid phrases like "we will now turn to", "let us examine"

IMPORTANT:

Write ONLY the transition text, nothing else
No metadata, no labels, no explanations
Each sentence should be ~14-28 words maximum
Be brief but meaningful
Maintain the documentary tone"""
  payload = {
      "type": "transition",
      "version": TRANSITION_PROMPT_VERSION,
      "language": language,
      "prev_preview": prev_end[:400],
      "next_preview": next_start[:400],
  }
  cached = self.cache.load(payload)
  if cached:
      return GeneratedText(text=cached.get("text", ""), provenance="gpt", prompt_hash=cached.get("cache_key"))

  try:
      response = self.client.chat.completions.create(
          model=self.cfg.model,
          messages=[
              {"role": "system", "content": "You are a professional documentary narrator specializing in creating smooth narrative transitions. You always use objective third-person voice."},
              {"role": "user", "content": user_prompt},
          ],
          temperature=self.cfg.temperature_transition,
          max_tokens=self.cfg.max_tokens_transition,
      )
      text = (response.choices[0].message.content or "").strip()
      cache_key = self.cache.save(payload, {"text": text})
      return GeneratedText(text=text, provenance="gpt", prompt_hash=cache_key)
  except Exception as e:
      # Fallback transitions without possessives
      fallback = "Na tomto pozad√≠ se plynule p≈ôesouv√°me k n√°sleduj√≠c√≠ ud√°losti." if language == 'CS' else "Against this backdrop, the narrative moves seamlessly to the next development."
      return GeneratedText(text=fallback, provenance="gpt", prompt_hash=None, meta={"error": str(e)})


</artifact>

3Ô∏è‚É£ historical_processor/narration_core/formatter.py
<artifact identifier="formatter-updated" type="application/vnd.ant.code" language="python" title="historical_processor/narration_core/formatter.py (Updated)">
# -*- coding: utf-8 -*-
from __future__ import annotations
import re
from typing import Dict, Optional, List
try:
from num2words import num2words  # type: ignore
except Exception:  # pragma: no cover - optional
num2words = None
from .types import FormatterConfig
from openai import OpenAI
class TextFormatter:
def init(self, cfg: FormatterConfig):
self.cfg = cfg
self.lang = cfg.language.upper()
# --- Public API ---
def format(self, text: str) -> str:
    t = text
    t = self._normalize_whitespace(t)
    t = self._remove_bracketed_citations(t)
    t = self._expand_abbreviations(t)
    t = self._convert_years(t)
    t = self._convert_numbers(t)
    t = self._normalize_ellipsis(t)
    t = self._normalize_quotes_and_dashes(t)
    t = self._ensure_sentence_termination(t)

    # üÜï NOV√â: Odstranƒõn√≠ posesiv a validace d√©lky vƒõt
    t = self._remove_possessive_pronouns(t)
    t = self._validate_sentence_length(t, max_words=30)

    if self.cfg.use_gpt_split or self.cfg.use_gpt_grammar:
        t = self._gpt_edit(t)
    else:
        t = self._split_long_sentences(t)

    t = self._final_cleanup(t)
    return t

# --- Steps ---
def _normalize_whitespace(self, text: str) -> str:
    # unify newlines and collapse multiple spaces
    t = text.replace("\r\n", "\n").replace("\r", "\n")
    t = re.sub(r"[\u200B\u200C\u200D\uFEFF]", "", t)  # remove zero width chars
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"\n\s*\n+", "\n\n", t)
    return t.strip()

def _remove_bracketed_citations(self, text: str) -> str:
    t = re.sub(r"\([A-Z√Åƒåƒé√âƒö√ç≈á√ì≈ò≈†≈§√ö≈Æ√ù≈Ω][a-z√°ƒçƒè√©ƒõ√≠≈à√≥≈ô≈°≈•√∫≈Ø√Ω≈æ]+,?\s+\d{4}\)", "", text)
    t = re.sub(r"\[[^\]]+\]", "", t)
    t = re.sub(r"(?i)podle\s+[A-Z√Åƒåƒé√âƒö√ç≈á√ì≈ò≈†≈§√ö≈Æ√ù≈Ω][a-z√°ƒçƒè√©ƒõ√≠≈à√≥≈ô≈°≈•√∫≈Ø√Ω≈æ]+a?", "", t)
    return t

def _expand_abbreviations(self, text: str) -> str:
    maps: Dict[str, Dict[str, str]] = {
        'CS': {
            'nap≈ô.': 'nap≈ô√≠klad', 'tzv.': 'takzvan√Ω', 'tj.': 'to jest', 'tzn.': 'to znamen√°',
            'atd.': 'a tak d√°le', 'apod.': 'a podobnƒõ', 'resp.': 'respektive', 'pop≈ô.': 'pop≈ô√≠padƒõ',
            'cca': 'cirka', 'ƒç.': 'ƒç√≠slo', 'str.': 'strana', 'odd.': 'odd√≠l', 'ods.': 'odstavec',
            'kpt.': 'kapit√°n', 'plk.': 'plukovn√≠k', 'gen.': 'gener√°l', 'dr.': 'doktor', 'prof.': 'profesor', 'ing.': 'in≈æen√Ωr'
        },
        'EN': {'e.g.': 'for example', 'i.e.': 'that is', 'etc.': 'et cetera', 'vs.': 'versus', 'ca.': 'circa', 'approx.': 'approximately', 'Dr.': 'Doctor', 'Prof.': 'Professor'},
        'DE': {'z.B.': 'zum Beispiel', 'd.h.': 'das hei√üt', 'usw.': 'und so weiter', 'bzw.': 'beziehungsweise', 'ca.': 'circa', 'Dr.': 'Doktor', 'Prof.': 'Professor'},
        'ES': {'p.ej.': 'por ejemplo', 'etc.': 'etc√©tera', 'aprox.': 'aproximadamente', 'Dr.': 'Doctor', 'Prof.': 'Profesor'},
        'FR': {'p.ex.': 'par exemple', 'c.-√†-d.': "c'est-√†-dire", 'etc.': 'et cetera', 'env.': 'environ', 'Dr.': 'Docteur', 'Prof.': 'Professeur'},
    }
    m = maps.get(self.lang, maps['CS'])
    t = text
    for abbr, full in m.items():
        t = re.sub(rf"\b{re.escape(abbr)}\b", full, t)
    return t

def _convert_years(self, text: str) -> str:
    if not num2words:
        return text
    def repl(m):
        y = int(m.group(0))
        try:
            out = num2words(y, lang=self._n2w_lang())
            return out.replace(" ", "").replace("-", "").replace("\u200B", "")
        except Exception:
            return m.group(0)
    return re.sub(r"\b(1\d{3}|20\d{2})\b", repl, text)

def _convert_numbers(self, text: str) -> str:
    if not num2words:
        return text
    def repl(m):
        n = int(m.group(0))
        if 1 <= n <= 999:
            try:
                out = num2words(n, lang=self._n2w_lang())
                return out.replace(" ", "").replace("-", "").replace("\u200B", "")
            except Exception:
                return m.group(0)
        return m.group(0)
    return re.sub(r"(?<!\d)\b([1-9]\d{0,2})(?!\d)", repl, text)

def _normalize_ellipsis(self, text: str) -> str:
    t = text
    if self.cfg.use_single_ellipsis_char:
        t = t.replace("...", "‚Ä¶")
    # collapse multiple ellipses
    t = re.sub(r"‚Ä¶{2,}", "‚Ä¶", t)
    return t

def _normalize_quotes_and_dashes(self, text: str) -> str:
    t = text
    # normalize dashes
    if self.cfg.use_en_dash_for_aside:
        t = re.sub(r"\s+-\s+", " ‚Äì ", t)
    # basic quotes normalization (language-aware minimal)
    if self.lang in ("CS", "DE"):
        t = t.replace('"', '"')  # no-op placeholder, real mapping might be added later
    elif self.lang == "EN":
        t = t.replace('"', '"')
    elif self.lang in ("ES", "FR"):
        t = t.replace('"', '"')
    return t

def _ensure_sentence_termination(self, text: str) -> str:
    # ensure each paragraph ends with terminal punctuation without relying on \p unicode classes
    paragraphs = re.split(r"\n\n+", text.strip())
    fixed = []
    for p in paragraphs:
        p = p.strip()
        if not p:
            continue
        last = p[-1]
        if last not in ".!?‚Ä¶":
            p = p + "."
        fixed.append(p)
    t = "\n\n".join(fixed)
    # ensure one space after punctuation, none before
    t = re.sub(r"\s+([.,!?;:])", r"\1", t)
    t = re.sub(r"([.,!?;:])\s*", r"\1 ", t)
    return t

def _remove_possessive_pronouns(self, text: str) -> str:
    """Remove first-person plural possessive pronouns (NOV√Å METODA)."""

    replacements = {
        'CS': [
            (r'\bna≈°eho\s+', ''),  # na≈°eho seri√°lu -> seri√°lu
            (r'\bna≈°e\s+', ''),     # na≈°e epizoda -> epizoda
            (r'\bn√°≈°\s+', ''),      # n√°≈° dokument -> dokument
            (r'\bv\s+na≈°em\s+dokumentu\b', 'v tomto dokumentu'),
            (r'\bv\s+na≈°√≠\s+s√©rii\b', 'v t√©to s√©rii'),
            (r'\bv\s+na≈°em\s+seri√°lu\b', 'v tomto seri√°lu'),
            (r'\bdokument√°rn√≠ho\s+seri√°lu\s+na≈°eho\b', 'dokument√°rn√≠ho seri√°lu'),
            (r'\bs√©ri√≠m\s+na≈°im\b', 'tƒõmto s√©ri√≠m'),
        ],
        'EN': [
            (r'\bour\s+series\b', 'the series'),
            (r'\bour\s+documentary\b', 'the documentary'),
            (r'\bour\s+episode\b', 'the episode'),
            (r'\bin\s+our\s+', 'in the '),
            (r'\bof\s+our\s+', 'of the '),
            (r'\bour\s+', 'the '),  # fallback: our X -> the X
        ],
        'DE': [
            (r'\bunserer\s+Serie\b', 'der Serie'),
            (r'\bunserem\s+Dokumentarfilm\b', 'dem Dokumentarfilm'),
            (r'\bunsere\s+Episode\b', 'die Episode'),
            (r'\bunser\s+', 'der '),
            (r'\bunsere\s+', 'die '),
            (r'\bunserem\s+', 'dem '),
        ],
        'ES': [
            (r'\bnuestra\s+serie\b', 'la serie'),
            (r'\bnuestro\s+documental\b', 'el documental'),
            (r'\bnuestro\s+episodio\b', 'el episodio'),
            (r'\bnuestro\s+', 'el '),
            (r'\bnuestra\s+', 'la '),
            (r'\bnuestros\s+', 'los '),
        ],
        'FR': [
            (r'\bnotre\s+s√©rie\b', 'la s√©rie'),
            (r'\bnotre\s+documentaire\b', 'le documentaire'),
            (r'\bnotre\s+√©pisode\b', "l'√©pisode"),
            (r'\bnotre\s+', 'le '),
            (r'\bnos\s+', 'les '),
        ]
    }

    t = text
    for pattern, repl in replacements.get(self.lang, []):
        t = re.sub(pattern, repl, t, flags=re.IGNORECASE)

    # Clean up potential double spaces
    t = re.sub(r'\s+', ' ', t)

    return t

def _validate_sentence_length(self, text: str, max_words: int = 30) -> str:
    """Split sentences longer than max_words at natural breaks (NOV√Å METODA)."""
    sentences = re.split(r'(?<=[.!?‚Ä¶])\s+', text)
    fixed = []

    for s in sentences:
        words = s.split()
        if len(words) <= max_words:
            fixed.append(s)
            continue

        # Split long sentence at natural breaks
        parts = self._intelligent_split(words, max_words)
        fixed.extend(parts)

    return ' '.join(fixed)

def _split_long_sentences(self, text: str) -> str:
    # heuristic: split sentences longer than ~30 words at commas or conjunctions
    sentences = re.split(r"(?<=[.!?‚Ä¶])\s+", text)
    out = []
    for s in sentences:
        words = s.split()
        if len(words) <= 30:
            out.append(s)
            continue
        out.extend(self._intelligent_split(words, max_words=30))
    return " ".join(out)

def _intelligent_split(self, words: List[str], max_words: int = 30) -> List[str]:
    """Split near middle at comma or conjunction"""
    n = len(words)
    if n <= max_words:
        return [" ".join(words)]

    mid = n // 2

    # prefer comma near middle
    comma_idx = None
    for i in range(max(10, mid - 10), min(n - 10, mid + 10)):
        if ',' in words[i]:
            comma_idx = i
            break

    if comma_idx is not None:
        first_part = " ".join(words[:comma_idx + 1]).rstrip(", ")
        second_part = " ".join(words[comma_idx + 1:])
        return [first_part + ".", self._capitalize_first(second_part)]

    # fallback: split at nearest conjunction
    conj = {"a", "ale", "v≈°ak", "zat√≠mco", "proto≈æe", "nebo", "and", "but", "while", "because", "or"}
    best = None
    for i in range(max(10, mid - 10), min(n - 10, mid + 10)):
        token = words[i].strip(",").lower()
        if token in conj:
            best = i
            break

    if best is not None:
        first_part = " ".join(words[:best]).rstrip(", ")
        second_part = " ".join(words[best:])
        return [first_part + ".", self._capitalize_first(second_part)]

    # last resort: hard split
    first_part = " ".join(words[:mid]).rstrip(", ")
    second_part = " ".join(words[mid:])
    return [first_part + ".", self._capitalize_first(second_part)]

def _capitalize_first(self, s: str) -> str:
    return s[:1].upper() + s[1:] if s else s

def _final_cleanup(self, text: str) -> str:
    t = text
    # collapse duplicate punctuation
    t = re.sub(r"([.!?]){2,}", r"\1", t)
    # ensure spacing rules
    t = re.sub(r"\s+([.,!?;:])", r"\1", t)
    t = re.sub(r"([.,!?;:])\s*", r"\1 ", t)
    # collapse multiple newlines and spaces
    t = re.sub(r"\n\s*\n+", "\n\n", t)
    t = re.sub(r"\s+", " ", t)
    return t.strip()

# --- GPT assisted editing (optional) ---
def _gpt_edit(self, text: str) -> str:
    api_key = self.cfg.api_key
    if not api_key:
        # fallback to offline splitting if API missing
        return self._split_long_sentences(text)
    client = OpenAI(api_key=api_key)
    lang_full = {
        'CS': 'Czech', 'EN': 'English', 'DE': 'German', 'ES': 'Spanish', 'FR': 'French'
    }.get(self.lang, 'Czech')

    prompt = (
        f"You are a professional {lang_full} text editor for documentary narration.\n"
        "Rules:\n"
        "1) Split only sentences longer than 30 words into 2 shorter sentences; keep meaning.\n"
        "2) Fix only obvious grammar/spelling/punctuation errors conservatively.\n"
        "3) Keep documentary, neutral tone.\n"
        "4) Do not add or remove factual content.\n"
        "5) NEVER use possessive pronouns like 'our', 'my' (Czech: 'n√°≈°', 'na≈°e', 'na≈°eho').\n"
        "6) Use objective third-person voice throughout.\n"
        "Return ONLY the processed text."
    )
    try:
        resp = client.chat.completions.create(
            model=self.cfg.model,
            messages=[
                {"role": "system", "content": "You are a conservative text editor who maintains objective third-person voice."},
                {"role": "user", "content": prompt + "\n\nTEXT:\n" + text},
            ],
            temperature=min(self.cfg.temperature_split, self.cfg.temperature_grammar),
            max_tokens=20000,
        )
        out = (resp.choices[0].message.content or "").strip()
        return out if out else text
    except Exception:
        return self._split_long_sentences(text)

def _n2w_lang(self) -> str:
    mapping = {'CS': 'cs', 'EN': 'en', 'DE': 'de', 'ES': 'es', 'FR': 'fr'}
    return mapping.get(self.lang, 'cs')
</artifact>

4Ô∏è‚É£ historical_processor/narration_core/validator.py (voliteln√© roz≈°√≠≈ôen√≠)
<artifact identifier="validator-updated" type="application/vnd.ant.code" language="python" title="historical_processor/narration_core/validator.py (Updated with sentence length check)">
# -*- coding: utf-8 -*-
from __future__ import annotations
import re
from dataclasses import dataclass
from typing import List, Set
META_PHRASES = {
"CS": [
"v t√©to ƒç√°sti", "v t√©to kapitole", "v n√°sleduj√≠c√≠ ƒç√°sti", "nyn√≠ se pod√≠v√°me", "tato sekce",
"v t√©to epizodƒõ", "v n√°sleduj√≠c√≠ epizodƒõ", "v p≈ô√≠≈°t√≠ ƒç√°sti",
],
"EN": [
"in this section", "in the following section", "now we will look", "this section",
"in this episode", "in the next episode",
],
"DE": [
"in diesem abschnitt", "im folgenden abschnitt", "nun betrachten wir", "diese sektion",
"in dieser episode", "inRetry






        PJContinue <artifact identifier="validator-updated" type="application/vnd.ant.code" language="python" title="historical_processor/narration_core/validator.py (Updated with sentence length check)">
# -*- coding: utf-8 -*-
from __future__ import annotations
import re
from dataclasses import dataclass
from typing import List, Set
META_PHRASES = {
"CS": [
"v t√©to ƒç√°sti", "v t√©to kapitole", "v n√°sleduj√≠c√≠ ƒç√°sti", "nyn√≠ se pod√≠v√°me", "tato sekce",
"v t√©to epizodƒõ", "v n√°sleduj√≠c√≠ epizodƒõ", "v p≈ô√≠≈°t√≠ ƒç√°sti",
],
"EN": [
"in this section", "in the following section", "now we will look", "this section",
"in this episode", "in the next episode",
],
"DE": [
"in diesem abschnitt", "im folgenden abschnitt", "nun betrachten wir", "diese sektion",
"in dieser episode", "in der n√§chsten episode",
],
"ES": [
"en esta secci√≥n", "en la siguiente secci√≥n", "ahora veremos", "esta secci√≥n",
"en este episodio", "en el pr√≥ximo episodio",
],
"FR": [
"dans cette section", "dans la section suivante", "nous allons maintenant", "cette section",
"dans cet √©pisode", "dans le prochain √©pisode",
],
}
POSSESSIVE_PATTERNS = {
"CS": [r'\bn√°≈°\b', r'\bna≈°e\b', r'\bna≈°eho\b', r'\bna≈°√≠\b', r'\bna≈°em\b'],
"EN": [r'\bour\b', r'\bours\b', r'\bmy\b', r'\bmine\b'],
"DE": [r'\bunser\b', r'\bunsere\b', r'\bunserem\b', r'\bunserer\b'],
"ES": [r'\bnuestro\b', r'\bnuestra\b', r'\bnuestros\b', r'\bnuestras\b'],
"FR": [r'\bnotre\b', r'\bnos\b', r'\bmon\b', r'\bma\b', r'\bmes\b'],
}
@dataclass
class ValidationResult:
ok: bool
reasons: List[str]
class TransitionQualityValidator:
"""Validate that a transition is smooth, logical, and non-meta.
Rules:
- 1‚Äì2 sentences
- avoid meta phrases (per language)
- include at least one anchor (if anchors exist in prev/next): year/entity/keyword overlap
- avoid copying long spans from prev/next (>50% token overlap)
- no possessive pronouns (NEW)
- sentences should be 14-28 words (NEW)
"""

SENT_SPLIT = re.compile(r"(?<=[.!?])\s+")
YEAR_RE = re.compile(r"\b(1\d{3}|20\d{2})\b")

def __init__(self, language: str):
    self.lang = language.upper()
    self.meta_phrases = [p.lower() for p in META_PHRASES.get(self.lang, META_PHRASES["EN"])]
    self.possessive_patterns = POSSESSIVE_PATTERNS.get(self.lang, POSSESSIVE_PATTERNS["EN"])

def validate(self, prev: str, next_: str, transition: str) -> ValidationResult:
    reasons: List[str] = []

    # 1) sentence count 1‚Äì2
    sentences = self._split_sentences(transition)
    if len(sentences) == 0:
        reasons.append("empty_transition")
    if len(sentences) > 2:
        reasons.append("too_many_sentences")

    low = transition.strip().lower()

    # 2) meta phrases
    if any(p in low for p in self.meta_phrases):
        reasons.append("contains_meta_phrase")

    # 3) anchors
    anchors_prev = self._anchors(prev)
    anchors_next = self._anchors(next_)
    anchors_any = anchors_prev | anchors_next
    if anchors_any:
        if not self._contains_any_anchor(low, anchors_any):
            reasons.append("missing_anchor")

    # 4) duplication check (Jaccard overlap > 0.5 with prev or next)
    toks_tr = self._tokens(low)
    if toks_tr:
        for ctx in (prev, next_):
            j = self._jaccard(toks_tr, self._tokens(ctx.lower()))
            if j > 0.5:
                reasons.append("too_similar_to_context")
                break

    # üÜï 5) Check for possessive pronouns
    for pattern in self.possessive_patterns:
        if re.search(pattern, transition, flags=re.IGNORECASE):
            reasons.append("contains_possessive_pronoun")
            break

    # üÜï 6) Check sentence length (14-28 words per sentence)
    for sent in sentences:
        word_count = len(sent.split())
        if word_count < 14:
            reasons.append(f"sentence_too_short_{word_count}_words")
        elif word_count > 28:
            reasons.append(f"sentence_too_long_{word_count}_words")

    return ValidationResult(ok=len(reasons) == 0, reasons=reasons)

# --- helpers ---
def _split_sentences(self, text: str) -> List[str]:
    parts = re.split(r"(?<=[.!?])\s+", text.strip())
    return [p for p in parts if p]

def _tokens(self, text: str) -> Set[str]:
    # simplified tokenization without \p unicode classes
    return {t for t in re.findall(r"[A-Za-z√Ä-√ñ√ò-√∂√∏-√øƒÄ-≈æ·∏Ä-·ªø0-9]+", text) if t}

def _anchors(self, text: str) -> Set[str]:
    s = set()
    # years
    s.update(self.YEAR_RE.findall(text))
    # capitalized words (simple heuristic, ignore beginning of sentence effects)
    for m in re.finditer(r"\b[A-Z√Åƒåƒé√âƒö√ç≈á√ì≈ò≈†≈§√ö≈Æ√ù≈Ω][a-z√°ƒçƒè√©ƒõ√≠≈à√≥≈ô≈°≈•√∫≈Ø√Ω≈æ]+\b", text):
        s.add(m.group(0))
    return s

def _contains_any_anchor(self, low_text: str, anchors: Set[str]) -> bool:
    for a in anchors:
        if a.lower() in low_text:
            return True
    return False

def _jaccard(self, a: Set[str], b: Set[str]) -> float:
    if not a or not b:
        return 0.0
    inter = len(a & b)
    union = len(a | b)
    return inter / max(1, union)
class SegmentQualityValidator:
"""Validate full segment narration (NEW CLASS for segment validation)."""
def __init__(self, language: str):
    self.lang = language.upper()
    self.possessive_patterns = POSSESSIVE_PATTERNS.get(self.lang, POSSESSIVE_PATTERNS["EN"])

def validate(self, text: str, max_sentence_words: int = 30) -> ValidationResult:
    reasons: List[str] = []

    # 1) Check for possessive pronouns
    for pattern in self.possessive_patterns:
        if re.search(pattern, text, flags=re.IGNORECASE):
            reasons.append("contains_possessive_pronoun")
            break

    # 2) Check sentence length
    sentences = re.split(r"(?<=[.!?])\s+", text.strip())
    for i, sent in enumerate(sentences, 1):
        word_count = len(sent.split())
        if word_count > max_sentence_words:
            reasons.append(f"sentence_{i}_too_long_{word_count}_words")

    # 3) Check for meta-narrative phrases
    meta_phrases = META_PHRASES.get(self.lang, META_PHRASES["EN"])
    low = text.lower()
    for phrase in meta_phrases:
        if phrase in low:
            reasons.append(f"contains_meta_phrase_{phrase}")

    return ValidationResult(ok=len(reasons) == 0, reasons=reasons)
</artifact>

üìã Souhrn zmƒõn
‚úÖ segment_prompt.txt

‚ûï Sekce "CRITICAL STYLE REQUIREMENTS"
‚ûï Explicitn√≠ z√°kaz posesiv pro v≈°echny jazyky
‚ûï Po≈æadavek max 30 slov na vƒõtu
‚ûï Validaƒçn√≠ pole max_sentence_length a possessive_pronouns_used

‚úÖ generator.py

‚ûï Sekce "CRITICAL STYLE RULES" v IntroGenerator
‚ûï Sekce "CRITICAL STYLE RULES" v TransitionGenerator
‚ûï Aktualizace system prompts pro zamezen√≠ posesiv
‚ûï Fallback texty bez posesiv
üîÑ Zv√Ω≈°en√° verze prompt≈Ø (v1 ‚Üí v2)

‚úÖ formatter.py

‚ûï _remove_possessive_pronouns() ‚Äì odstran√≠ "n√°≈°/our/unser" atd.
‚ûï _validate_sentence_length() ‚Äì rozdƒõl√≠ vƒõty del≈°√≠ ne≈æ 30 slov
üîÑ Integrace obou metod do hlavn√≠ format() metody
üîÑ Aktualizace _gpt_edit() pro z√°kaz posesiv

‚úÖ validator.py

‚ûï POSSESSIVE_PATTERNS ‚Äì regex vzory pro detekci posesiv
‚ûï Kontrola posesiv v TransitionQualityValidator
‚ûï Kontrola d√©lky vƒõt (14-28 slov pro transitions)
‚ûï Nov√° t≈ô√≠da SegmentQualityValidator pro validaci cel√Ωch segment≈Ø


üöÄ Jak nasadit zmƒõny
1Ô∏è‚É£ Z√°loha p≈Øvodn√≠ch soubor≈Ø
bashcp B_core/templates/segment_prompt.txt B_core/templates/segment_prompt.txt.bak
cp historical_processor/narration_core/generator.py historical_processor/narration_core/generator.py.bak
cp historical_processor/narration_core/formatter.py historical_processor/narration_core/formatter.py.bak
cp historical_processor/narration_core/validator.py historical_processor/narration_core/validator.py.bak
2Ô∏è‚É£ Nahraƒè soubory nov√Ωmi verzemi
Zkop√≠ruj obsah artefakt≈Ø do odpov√≠daj√≠c√≠ch soubor≈Ø.
3Ô∏è‚É£ Otestuj na mal√©m vzorku
bash# Vygeneruj jeden segment pro test
python B_core/generate_prompts.py --topic "test_topic" --language CS
python claude_generator/runner_cli.py --topic "test_topic" --language CS --episodes ep01
python historical_processor/runner_cli.py --input-dir outputs/narration/test_topic/CS/ep01 --episode-mode
4Ô∏è‚É£ Zkontroluj v√Ωstup
bash# Zkontroluj, ≈æe v√Ωstup neobsahuje "na≈°eho" a m√° kr√°tk√© vƒõty
cat outputs/postprocess/test_topic/CS/ep01/episode_merged.txt
```

---

## üéØ Oƒçek√°van√© v√Ωsledky

### ‚ùå **P≈òED (≈°patnƒõ):**
```
V√≠tejte v prvn√≠m d√≠lu na≈°eho dokument√°rn√≠ho seri√°lu "Bitva o dukelsk√Ω pr≈Øsmyk",
kde se budeme zab√Ωvat kl√≠ƒçov√Ωmi ud√°lostmi, kter√© ovlivnily pr≈Øbƒõh cel√© operace
a mƒõly z√°sadn√≠ dopad na dal≈°√≠ v√Ωvoj situace na v√Ωchodn√≠ frontƒõ bƒõhem z√°vƒõreƒçn√©
f√°ze druh√© svƒõtov√© v√°lky v roce 1944.
```

### ‚úÖ **PO (spr√°vnƒõ):**
```
V√≠tejte v prvn√≠m d√≠lu dokument√°rn√≠ho seri√°lu "Bitva o dukelsk√Ω pr≈Øsmyk".
Tato epizoda se zab√Ωv√° kl√≠ƒçov√Ωmi ud√°lostmi, kter√© ovlivnily pr≈Øbƒõh operace.
Tyto ud√°losti mƒõly z√°sadn√≠ dopad na v√Ωvoj situace na v√Ωchodn√≠ frontƒõ
bƒõhem z√°vƒõreƒçn√© f√°ze druh√© svƒõtov√© v√°lky v roce 1944.
Zmƒõny:

‚ùå na≈°eho ‚Üí ‚úÖ odstranƒõno
‚ùå 1 dlouh√° vƒõta (45 slov) ‚Üí ‚úÖ 3 kr√°tk√© vƒõty (12, 14, 19 slov)
‚úÖ Objektivn√≠ third-person hlas
‚úÖ Plynul√Ω dokument√°rn√≠ styl


üîç Debugging
Pokud st√°le vid√≠≈° "na≈°eho" nebo dlouh√© vƒõty:

Zkontroluj cache ‚Äì narration_core pou≈æ√≠v√° cache, sma≈æ .cache/narration_core/
Zkontroluj verzi prompt≈Ø ‚Äì mƒõly by b√Ωt v2
Zkontroluj logy ‚Äì pod√≠vej se do outputs/*/logs/ na chyby
Manu√°ln√≠ test formateru:

pythonfrom historical_processor.narration_core.formatter import TextFormatter
from historical_processor.narration_core.types import FormatterConfig

cfg = FormatterConfig(language='CS', use_gpt_split=False, use_gpt_grammar=False)
fmt = TextFormatter(cfg)

test_text = "V√≠tejte v prvn√≠m d√≠lu na≈°eho dokument√°rn√≠ho seri√°lu..."
result = fmt.format(test_text)
print(result)
